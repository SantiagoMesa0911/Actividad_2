{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e0d5436-aae7-49ba-9c4e-30e8f6d47ea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONFIGURACIÓN DATABRICKS SERVERLESS ===\nVersión de Spark: 4.0.0\n\n=== INFORMACIÓN DEL SISTEMA ===\nVersión de Python: 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0]\n\n=== CONFIGURACIÓN DISPONIBLE ===\nspark.sql.adaptive.enabled: No disponible\nspark.databricks.clusterUsageTags.clusterName: No disponible\nspark.databricks.clusterUsageTags.clusterId: 1124-024201-9yh8c1zi-v2n\n\n=== ESTRUCTURA DBFS ===\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/Volumes/</td><td>Volumes/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/Workspace/</td><td>Workspace/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/</td><td>databricks-datasets/</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/Volumes/",
         "Volumes/",
         0,
         0
        ],
        [
         "dbfs:/Workspace/",
         "Workspace/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/",
         "databricks-datasets/",
         0,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n=== FILE STORE ===\nFileStore no accesible: Public DBFS root is disabled. Access is denied on path: /FileStore\n\nJVM stacktrace:\njava.lang.UnsupportedOperationException\n\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.rejectOperation(DisabledDatabricksFileSystem.scala:31)\n\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.listStatus(DisabledDatabricksFileSystem.scala:96)\n\tat com.databricks.backend.daemon.data.client.DBFSV2.$anonfun$listStatus$2(DatabricksFileSystemV2.scala:177)\n\tat com.databricks.s3a.S3AExceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:64)\n\tat com.databricks.backend.daemon.data.client.DBFSV2.$anonfun$listStatus$1(DatabricksFileSystemV2.scala:174)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperationWithResultTags(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.backend.daemon.data.client.DBFSV2.listStatus(DatabricksFileSystemV2.scala:173)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.listStatus(DatabricksFileSystem.scala:157)\n\tat com.databricks.sql.io.LokiFileSystem.listStatus(LokiFileSystem.scala:234)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsWithLimit(DBUtilsCore.scala:439)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$4(DBUtilsCore.scala:303)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:189)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$3(DBUtilsCore.scala:303)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:184)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsImpl(DBUtilsCore.scala:302)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$1(DBUtilsCore.scala:274)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:100)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:100)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:100)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:100)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$recordDbutilsFsOp$6(DBUtilsCore.scala:166)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:166)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.ls(DBUtilsCore.scala:274)\n\tat com.databricks.sql.DbutilsFsSparkConnectBridgeImpl.ls(DbutilsFsSparkConnectBridgeImpl.scala:37)\n\tat com.databricks.sql.managedcatalog.command.ListDbutilsCommand.run(DbutilsFsCommands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:87)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:87)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:84)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:83)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:97)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:505)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:505)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:267)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:504)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$17(SQLExecution.scala:585)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$15(SQLExecution.scala:498)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:916)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$14(SQLExecution.scala:419)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$13(SQLExecution.scala:419)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:951)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:418)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:241)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:869)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:500)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:496)\n\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:438)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:494)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:597)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:589)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:530)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:530)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:506)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:589)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:589)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:394)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:399)\n\tat org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:432)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$11(Dataset.scala:245)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1077)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1077)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:239)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.handlePlan(SparkConnectPlanExecution.scala:112)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handlePlan(ExecuteThreadRunner.scala:374)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:281)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:140)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:52)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:138)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:584)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:584)\n"
     ]
    }
   ],
   "source": [
    "# Configuración para Databricks Serverless\n",
    "print(\"=== CONFIGURACIÓN DATABRICKS SERVERLESS ===\")\n",
    "print(f\"Versión de Spark: {spark.version}\")\n",
    "\n",
    "# Obtener configuración de manera compatible con Serverless\n",
    "print(\"\\n=== INFORMACIÓN DEL SISTEMA ===\")\n",
    "import sys\n",
    "print(f\"Versión de Python: {sys.version}\")\n",
    "\n",
    "# Información disponible en Serverless\n",
    "print(\"\\n=== CONFIGURACIÓN DISPONIBLE ===\")\n",
    "try:\n",
    "    # Estas configuraciones deberían funcionar en Serverless\n",
    "    configs_to_check = [\n",
    "        \"spark.sql.adaptive.enabled\",\n",
    "        \"spark.databricks.clusterUsageTags.clusterName\", \n",
    "        \"spark.databricks.clusterUsageTags.clusterId\"\n",
    "    ]\n",
    "    \n",
    "    for config in configs_to_check:\n",
    "        try:\n",
    "            value = spark.conf.get(config)\n",
    "            print(f\"{config}: {value}\")\n",
    "        except:\n",
    "            print(f\"{config}: No disponible\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Algunas configuraciones no disponibles en Serverless: {e}\")\n",
    "\n",
    "print(\"\\n=== ESTRUCTURA DBFS ===\")\n",
    "try:\n",
    "    display(dbutils.fs.ls(\"/\"))\n",
    "except Exception as e:\n",
    "    print(f\"No se puede acceder a DBFS root: {e}\")\n",
    "\n",
    "# Verificar acceso a FileStore\n",
    "print(\"\\n=== FILE STORE ===\")\n",
    "try:\n",
    "    display(dbutils.fs.ls(\"/FileStore\"))\n",
    "except Exception as e:\n",
    "    print(f\"FileStore no accesible: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebc1062f-ecf3-461e-89a1-94621ac3c5e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Conectado a tabla 'default.Train'\nTotal registros: 891\n\nPrimeros 5 registros:\n+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| NULL|       S|\n|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| NULL|       S|\n|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| NULL|       S|\n+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Tu tabla YA EXISTE - trabajemos directamente con ella\n",
    "df = spark.table(\"default.Train\")\n",
    "\n",
    "print(\"✅ Conectado a tabla 'default.Train'\")\n",
    "print(f\"Total registros: {df.count()}\")\n",
    "print(\"\\nPrimeros 5 registros:\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6e74dcb-af4a-41a8-8db0-68bb027afd43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ESQUEMA DE LA TABLA ===\nroot\n |-- PassengerId: long (nullable = true)\n |-- Survived: long (nullable = true)\n |-- Pclass: long (nullable = true)\n |-- Name: string (nullable = true)\n |-- Sex: string (nullable = true)\n |-- Age: double (nullable = true)\n |-- SibSp: long (nullable = true)\n |-- Parch: long (nullable = true)\n |-- Ticket: string (nullable = true)\n |-- Fare: double (nullable = true)\n |-- Cabin: string (nullable = true)\n |-- Embarked: string (nullable = true)\n\n\n=== DESCRIBE TABLE ===\n+-----------+---------+-------+\n|   col_name|data_type|comment|\n+-----------+---------+-------+\n|PassengerId|   bigint|   NULL|\n|   Survived|   bigint|   NULL|\n|     Pclass|   bigint|   NULL|\n|       Name|   string|   NULL|\n|        Sex|   string|   NULL|\n|        Age|   double|   NULL|\n|      SibSp|   bigint|   NULL|\n|      Parch|   bigint|   NULL|\n|     Ticket|   string|   NULL|\n|       Fare|   double|   NULL|\n|      Cabin|   string|   NULL|\n|   Embarked|   string|   NULL|\n+-----------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# ACTUALIZA ESTA RUTA con la que te dio Databricks al subir el archivo\n",
    "print(\"=== ESQUEMA DE LA TABLA ===\")\n",
    "df.printSchema()\n",
    "\n",
    "# También podemos verlo con SQL\n",
    "print(\"\\n=== DESCRIBE TABLE ===\")\n",
    "spark.sql(\"DESCRIBE default.Train\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0af85bf-f1d5-4d3a-9de2-d03691d87ed4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VALIDACIONES BÁSICAS ===\n1. Total de registros: 891\n2. Columnas: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n3. Estadísticas descriptivas:\n+-------+-----------------+------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+------------------+-----+--------+\n|summary|      PassengerId|          Survived|            Pclass|                Name|   Sex|               Age|             SibSp|              Parch|            Ticket|              Fare|Cabin|Embarked|\n+-------+-----------------+------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+------------------+-----+--------+\n|  count|              891|               891|               891|                 891|   891|               714|               891|                891|               891|               891|  204|     889|\n|   mean|            446.0|0.3838383838383838| 2.308641975308642|                NULL|  NULL| 29.69911764705882|0.5230078563411896|0.38159371492704824|260318.54916792738|  32.2042079685746| NULL|    NULL|\n| stddev|257.3538420152301|0.4865924542648585|0.8360712409770513|                NULL|  NULL|14.526497332334044|1.1027434322934275| 0.8060572211299559|471609.26868834946|49.693428597180905| NULL|    NULL|\n|    min|                1|                 0|                 1| Abbing, Mr. Anthony|female|              0.42|                 0|                  0|            110152|               0.0|  A10|       C|\n|    max|              891|                 1|                 3|van Melkebeke, Mr...|  male|              80.0|                 8|                  6|         WE/P 5735|          512.3292|    T|       S|\n+-------+-----------------+------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+------------------+-----+--------+\n\n4. Distribución por clase:\n+------+-----+\n|Pclass|count|\n+------+-----+\n|     3|  491|\n|     1|  216|\n|     2|  184|\n+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "print(\"=== VALIDACIONES BÁSICAS ===\")\n",
    "\n",
    "# 1. Conteo total\n",
    "total_count = df.count()\n",
    "print(f\"1. Total de registros: {total_count}\")\n",
    "\n",
    "# 2. Columnas disponibles\n",
    "print(f\"2. Columnas: {df.columns}\")\n",
    "\n",
    "# 3. Estadísticas descriptivas\n",
    "print(\"3. Estadísticas descriptivas:\")\n",
    "df.describe().show()\n",
    "\n",
    "# 4. Conteo por clase\n",
    "print(\"4. Distribución por clase:\")\n",
    "df.groupBy(\"Pclass\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e8ca445-a6f3-4090-93c4-0797550c92d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vista temporal 'titanic_view' creada\nAhora puedes usar consultas SQL con: spark.sql('...') o %sql\n"
     ]
    }
   ],
   "source": [
    "# Crear vista temporal para consultas SQL\n",
    "df.createOrReplaceTempView(\"titanic_view\")\n",
    "\n",
    "print(\"✅ Vista temporal 'titanic_view' creada\")\n",
    "print(\"Ahora puedes usar consultas SQL con: spark.sql('...') o %sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eec81a6d-c57a-4eb8-978b-95a8662b122c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Pclass</th><th>total</th><th>tasa_supervivencia</th></tr></thead><tbody><tr><td>1</td><td>216</td><td>0.6296296296296297</td></tr><tr><td>2</td><td>184</td><td>0.47282608695652173</td></tr><tr><td>3</td><td>491</td><td>0.24236252545824846</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         216,
         0.6296296296296297
        ],
        [
         2,
         184,
         0.47282608695652173
        ],
        [
         3,
         491,
         0.24236252545824846
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "Pclass",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "total",
            "nullable": false,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "tasa_supervivencia",
            "nullable": true,
            "type": "double"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 12
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Pclass",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "total",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "tasa_supervivencia",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- Consultas SQL funcionan perfectamente en Serverless\n",
    "-- 1. Metadatos de la tabla\n",
    "DESCRIBE default.Train;\n",
    "\n",
    "-- 2. Estadísticas básicas\n",
    "SELECT \n",
    "    COUNT(*) as total_pasajeros,\n",
    "    AVG(Survived) as tasa_supervivencia,\n",
    "    AVG(Age) as edad_promedio,\n",
    "    AVG(Fare) as tarifa_promedio\n",
    "FROM default.Train;\n",
    "\n",
    "-- 3. Supervivencia por clase\n",
    "SELECT \n",
    "    Pclass,\n",
    "    COUNT(*) as total,\n",
    "    AVG(Survived) as tasa_supervivencia\n",
    "FROM default.Train \n",
    "GROUP BY Pclass \n",
    "ORDER BY Pclass;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ebe41b1-6d78-43b7-8008-3ac21d21a047",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ANÁLISIS CON PYSPARK ===\n+------+------+-----+-------------------+\n|Pclass|   Sex|total| tasa_supervivencia|\n+------+------+-----+-------------------+\n|     1|female|   94| 0.9680851063829787|\n|     1|  male|  122|0.36885245901639346|\n|     2|female|   76| 0.9210526315789473|\n|     2|  male|  108| 0.1574074074074074|\n|     3|female|  144|                0.5|\n|     3|  male|  347|0.13544668587896252|\n+------+------+-----+-------------------+\n\n=== MISMO ANÁLISIS CON SQL ===\n+------+------+-----+-------------------+\n|Pclass|   Sex|total| tasa_supervivencia|\n+------+------+-----+-------------------+\n|     1|female|   94| 0.9680851063829787|\n|     1|  male|  122|0.36885245901639346|\n|     2|female|   76| 0.9210526315789473|\n|     2|  male|  108| 0.1574074074074074|\n|     3|female|  144|                0.5|\n|     3|  male|  347|0.13544668587896252|\n+------+------+-----+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# PySpark - Análisis por clase y género\n",
    "from pyspark.sql.functions import avg, count\n",
    "\n",
    "print(\"=== ANÁLISIS CON PYSPARK ===\")\n",
    "result_spark = df.groupBy(\"Pclass\", \"Sex\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total\"),\n",
    "        avg(\"Survived\").alias(\"tasa_supervivencia\")\n",
    "    ) \\\n",
    "    .orderBy(\"Pclass\", \"Sex\")\n",
    "\n",
    "result_spark.show()\n",
    "\n",
    "print(\"=== MISMO ANÁLISIS CON SQL ===\")\n",
    "result_sql = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        Pclass,\n",
    "        Sex,\n",
    "        COUNT(*) as total,\n",
    "        AVG(Survived) as tasa_supervivencia\n",
    "    FROM titanic_view \n",
    "    GROUP BY Pclass, Sex \n",
    "    ORDER BY Pclass, Sex\n",
    "\"\"\")\n",
    "result_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0f621bf-e0ae-43d6-a8c4-d6460a9fe877",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Comparación: SQL vs Spark\n",
    "\n",
    "### Ventajas de SQL\n",
    "- **Sintaxis familiar**: Muchos analistas de datos conocen SQL.\n",
    "- **Declarativo**: Especificas qué quieres, no cómo obtenerlo.\n",
    "- **Optimización**: El motor de SQL puede optimizar la consulta automáticamente.\n",
    "- **Integración**: Fácil integración con herramientas de BI.\n",
    "\n",
    "### Desventajas de SQL\n",
    "- **Limitaciones en transformaciones complejas**: Algunas transformaciones son más difíciles de expresar en SQL.\n",
    "- **Menos flexibilidad**: Para pipelines complejos, puede ser menos flexible que Spark.\n",
    "\n",
    "### Ventajas de Spark (PySpark)\n",
    "- **Escalabilidad**: Diseñado para big data y procesamiento distribuido.\n",
    "- **APIs ricas**: DataFrame y Dataset APIs para transformaciones complejas.\n",
    "- **UDFs**: Permite definir funciones personalizadas en Python, Scala, etc.\n",
    "- **Integración con MLlib**: Para machine learning.\n",
    "\n",
    "### Desventajas de Spark\n",
    "- **Curva de aprendizaje**: Requiere aprender nuevas APIs y conceptos.\n",
    "- **Configuración**: Puede requerir ajustes de rendimiento.\n",
    "- **Overhead**: Para consultas simples, puede ser más lento que SQL.\n",
    "\n",
    "### Conclusión\n",
    "En entornos como Databricks, podemos aprovechar lo mejor de ambos: usar SQL para consultas ad-hoc y PySpark para pipelines de datos más complejos. La elección depende del caso de uso y de la familiaridad del equipo con cada tecnología."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1deff6f5-0f02-4d25-9344-649509f2a9d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Diseño del Esquema - Dataset Titanic\n",
    "\n",
    "### Diccionario de Datos\n",
    "| Columna | Tipo | Descripción | Nulable |\n",
    "|---------|------|-------------|---------|\n",
    "| PassengerId | BIGINT | ID único del pasajero | NO |\n",
    "| Survived | BIGINT | Supervivencia (0=No, 1=Sí) | SÍ |\n",
    "| Pclass | BIGINT | Clase del ticket (1,2,3) | SÍ |\n",
    "| Name | STRING | Nombre completo | SÍ |\n",
    "| Sex | STRING | Género | SÍ |\n",
    "| Age | DOUBLE | Edad | SÍ |\n",
    "| SibSp | BIGINT | Hermanos/Esposos a bordo | SÍ |\n",
    "| Parch | BIGINT | Padres/Hijos a bordo | SÍ |\n",
    "| Ticket | STRING | Número de ticket | SÍ |\n",
    "| Fare | DOUBLE | Tarifa pagada | SÍ |\n",
    "| Cabin | STRING | Cabina | SÍ |\n",
    "| Embarked | STRING | Puerto de embarque | SÍ |\n",
    "\n",
    "**Llave primaria:** PassengerId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c72e3574-8337-4a06-9ce8-4045e56a95ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ANÁLISIS POR GRUPOS DE EDAD ===\nSupervivencia por Grupo de Edad:\n+-----------+-----+-------------------+------------------+\n|  age_group|total| tasa_supervivencia|   tarifa_promedio|\n+-----------+-----+-------------------+------------------+\n|       Niño|  113| 0.5398230088495575| 31.22079823008851|\n|     Adulto|  256|         0.41796875|   39.551611328125|\n|      Mayor|   74|0.36486486486486486| 46.36441486486488|\n|      Joven|  271| 0.3505535055350554|28.368094464944658|\n|Desconocido|  177| 0.2937853107344633|22.158566666666673|\n+-----------+-----+-------------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "print(\"=== ANÁLISIS POR GRUPOS DE EDAD ===\")\n",
    "\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Crear grupos de edad\n",
    "df_age_groups = df.withColumn(\n",
    "    \"age_group\",\n",
    "    when(col(\"Age\").isNull(), \"Desconocido\")\n",
    "    .when(col(\"Age\") < 18, \"Niño\")\n",
    "    .when(col(\"Age\") < 30, \"Joven\")\n",
    "    .when(col(\"Age\") < 50, \"Adulto\")\n",
    "    .otherwise(\"Mayor\")\n",
    ")\n",
    "\n",
    "# Análisis de supervivencia por grupo de edad\n",
    "age_analysis = df_age_groups.groupBy(\"age_group\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total\"),\n",
    "        avg(\"Survived\").alias(\"tasa_supervivencia\"),\n",
    "        avg(\"Fare\").alias(\"tarifa_promedio\")\n",
    "    ) \\\n",
    "    .orderBy(\"tasa_supervivencia\", ascending=False)\n",
    "\n",
    "print(\"Supervivencia por Grupo de Edad:\")\n",
    "age_analysis.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b57683bb-b5f2-4254-ae0c-116ec2d4f7d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>age_group</th><th>total</th><th>tasa_supervivencia</th><th>tarifa_promedio</th></tr></thead><tbody><tr><td>Niño</td><td>113</td><td>0.5398230088495575</td><td>31.22079823008851</td></tr><tr><td>Adulto</td><td>256</td><td>0.41796875</td><td>39.551611328125</td></tr><tr><td>Mayor</td><td>74</td><td>0.36486486486486486</td><td>46.36441486486488</td></tr><tr><td>Joven</td><td>271</td><td>0.3505535055350554</td><td>28.368094464944658</td></tr><tr><td>Desconocido</td><td>177</td><td>0.2937853107344633</td><td>22.158566666666673</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Niño",
         113,
         0.5398230088495575,
         31.22079823008851
        ],
        [
         "Adulto",
         256,
         0.41796875,
         39.551611328125
        ],
        [
         "Mayor",
         74,
         0.36486486486486486,
         46.36441486486488
        ],
        [
         "Joven",
         271,
         0.3505535055350554,
         28.368094464944658
        ],
        [
         "Desconocido",
         177,
         0.2937853107344633,
         22.158566666666673
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "age_group",
            "nullable": false,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "total",
            "nullable": false,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "tasa_supervivencia",
            "nullable": true,
            "type": "double"
           },
           {
            "metadata": {},
            "name": "tarifa_promedio",
            "nullable": true,
            "type": "double"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 23
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "age_group",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "tasa_supervivencia",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "tarifa_promedio",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- Análisis por grupos de edad con SQL\n",
    "SELECT \n",
    "    CASE \n",
    "        WHEN Age IS NULL THEN 'Desconocido'\n",
    "        WHEN Age < 18 THEN 'Niño'\n",
    "        WHEN Age < 30 THEN 'Joven' \n",
    "        WHEN Age < 50 THEN 'Adulto'\n",
    "        ELSE 'Mayor'\n",
    "    END as age_group,\n",
    "    COUNT(*) as total,\n",
    "    AVG(Survived) as tasa_supervivencia,\n",
    "    AVG(Fare) as tarifa_promedio\n",
    "FROM default.Train \n",
    "GROUP BY \n",
    "    CASE \n",
    "        WHEN Age IS NULL THEN 'Desconocido'\n",
    "        WHEN Age < 18 THEN 'Niño'\n",
    "        WHEN Age < 30 THEN 'Joven'\n",
    "        WHEN Age < 50 THEN 'Adulto'\n",
    "        ELSE 'Mayor'\n",
    "    END\n",
    "ORDER BY tasa_supervivencia DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a133a23-0094-48ed-b1bb-a4824512d03e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ANÁLISIS COMPLEJO: CLASE + GÉNERO + EMBARQUE ===\nAnálisis Multidimensional:\n+------+------+--------+---------------+-------------------+------------------+------------------+\n|Pclass|   Sex|Embarked|total_pasajeros| tasa_supervivencia|     edad_promedio|   tarifa_promedio|\n+------+------+--------+---------------+-------------------+------------------+------------------+\n|     1|female|       C|             43| 0.9767441860465116| 36.05263157894737| 115.6403093023256|\n|     1|female|       S|             48| 0.9583333333333334| 32.70454545454545| 99.02691041666664|\n|     1|  male|       C|             42|0.40476190476190477|40.111111111111114| 93.53670714285715|\n|     1|  male|       S|             79|0.35443037974683544|        41.8971875| 52.94994683544305|\n|     2|female|       S|             67| 0.9104477611940298| 29.71969696969697| 21.91268656716418|\n|     2|  male|       S|             97|0.15463917525773196|30.875888888888888|19.232474226804122|\n|     3|female|       C|             23| 0.6521739130434783|           14.0625|14.694926086956524|\n|     3|female|       Q|             33| 0.7272727272727273|             22.85|10.307833333333331|\n|     3|female|       S|             88|              0.375|23.223684210526315| 18.67007727272727|\n|     3|  male|       C|             43|0.23255813953488372|           25.0168| 9.352237209302325|\n|     3|  male|       Q|             39|0.07692307692307693|28.142857142857142| 11.92425128205128|\n|     3|  male|       S|            265|0.12830188679245283|26.574766355140188| 13.30714905660378|\n+------+------+--------+---------------+-------------------+------------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "print(\"=== ANÁLISIS COMPLEJO: CLASE + GÉNERO + EMBARQUE ===\")\n",
    "\n",
    "complex_analysis = (df\n",
    "    .groupBy(\"Pclass\", \"Sex\", \"Embarked\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_pasajeros\"),\n",
    "        avg(\"Survived\").alias(\"tasa_supervivencia\"),\n",
    "        avg(\"Age\").alias(\"edad_promedio\"),\n",
    "        avg(\"Fare\").alias(\"tarifa_promedio\")\n",
    "    )\n",
    "    .filter(col(\"total_pasajeros\") > 10)   # Filtrar grupos pequeños\n",
    "    .orderBy(\"Pclass\", \"Sex\", \"Embarked\")\n",
    ")\n",
    "\n",
    "print(\"Análisis Multidimensional:\")\n",
    "complex_analysis.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6033042841674576,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Actividad_2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}